{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 Lab 4: Bandits\n",
    "\n",
    "In this part, you will explore a few different strategies for solving the multi-armed bandit problem, a classic example used in reinforcement learning. The \"multi-armed bandit\" is a scenario where you must choose between multiple actions (the arms of the bandit), each with a different and unknown reward probability distribution. The goal is to maximize your rewards over time.\n",
    "\n",
    "The multi-armed bandit problem is named after a slot machine (or \"one-armed bandit\") but with multiple levers. Each lever has a different probability of paying out a reward, and you want to discover which lever offers the best payout rate with the least number of trials.\n",
    "\n",
    "## Mathematical Framework\n",
    "\n",
    "### Problem Setting\n",
    "Consider a scenario where you have a set of $K$ slot machines (arms of the bandit). Each machine provides a reward drawn from a probability distribution that is specific to that machine but unknown to the agent. When the agent selects an arm $i$, it receives a reward $R_i$ based on the arm's reward distribution.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Action ($a$)**: Pulling a lever of a specific slot machine.\n",
    "- **Reward ($R_a$)**: The payout received after taking action $a$.\n",
    "- **Value of an Action ($q_*(a)$)**: The expected reward from action $a$, given by $q_*(a) = \\mathbb{E}[R_a]$.\n",
    "\n",
    "### Objective:\n",
    "The goal is to maximize the sum of rewards over some time period, typically by learning to choose the best machine through trial and experience.\n",
    "\n",
    "## Strategies for Solving Bandit Problems\n",
    "\n",
    "### Greedy Algorithm\n",
    "The greedy algorithm always chooses the action that has the highest estimated reward based on the information available up to that point.\n",
    "\n",
    "#### Action Selection:\n",
    "$$A_t = \\underset{a}{\\mathrm{argmax}} \\, Q_t(a)$$\n",
    "where $Q_t(a)$ is the estimated value of action $a$ at time $t$.\n",
    "\n",
    "### Epsilon-Greedy Algorithm\n",
    "This algorithm introduces a small probability $\\epsilon$ that the agent will explore the environment rather than exploit it. This probability ensures that every action is sampled sufficiently often.\n",
    "\n",
    "#### Action Selection:\n",
    "$$A_t = \\begin{cases} \n",
    "\\text{Random action from } \\{1, \\dots, K\\} & \\text{excluding the greedy action with probability } \\epsilon \\\\\n",
    "\\underset{a}{\\mathrm{argmax}} \\, Q_t(a) & \\text{with probability } 1 - \\epsilon\n",
    "\\end{cases}$$\n",
    "\n",
    "$\\epsilon$ is the probability of exploration, which involves randomly choosing any other action **except** the greedy action. \n",
    "\n",
    "With $\\epsilon = 0$, the epsilon-greedy algorithm is equivalent to the greedy algorithm.\n",
    "\n",
    "### Update Rule:\n",
    "Regardless of the strategy (greedy or epsilon-greedy), after selecting an action $A_t$ and observing a reward $R_t$, the estimated value $Q_t(A_t)$ is updated as follows:\n",
    "\n",
    "#### Incremental Update Formula:\n",
    "$$Q_{t+1}(A_t) = Q_t(A_t) + \\alpha_t (R_t - Q_t(A_t))$$\n",
    "where $\\alpha_t$ is the learning rate, which might typically be set as $\\alpha_t = \\frac{1}{\\text{count}(A_t)}$, the reciprocal of the number of times action $A_t$ has been selected. This update step adjusts the action-value estimate towards the actual reward received, refining the estimate based on the latest information.\n",
    "\n",
    "## Applications of Bandit Algorithms\n",
    "Bandit algorithms have practical applications in various fields including:\n",
    "\n",
    "- **Internet Advertising**: Optimizing ad selections to maximize click-through rates.\n",
    "- **Clinical Trials**: Efficiently determining the most effective treatment options.\n",
    "- **Finance**: Portfolio optimization under uncertainty.\n",
    "- **Recommendation Systems**: Dynamically adjusting recommendations to align with user preferences.\n",
    "\n",
    "Understanding the theory and implementation of multi-armed bandits allows for the development of efficient decision-making processes that are critical in many areas of business and science. In this part of the lab, you will implement and compare the performance of the greedy and epsilon-greedy algorithms in solving the multi-armed bandit problem.\n",
    "\n",
    "## Setup and Initial Exploration\n",
    "\n",
    "### Libraries and Initial Setup\n",
    "\n",
    "Import the necessary libraries and configure the environment for visualization. Make sure you have `numpy`, `matplotlib`, `seaborn`, and `pandas` installed.\n",
    "\n",
    "Alternatively, you can create the conda environment using the provided `environment.yml` file for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package if you don't have it\n",
    "# !pip install -q numpy seaborn matplotlib pandas pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set plt style\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandit Setup\n",
    "\n",
    "We begin by defining the number of arms, trials, and plays. The number of arms represents the number of slot machines in the bandit problem. The number of trials is the total number of times the agent will select an arm, and the number of plays is the number of times each arm will be played in a trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "n_trials = 2000\n",
    "n_plays = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement Core Functions\n",
    "\n",
    "### Update Function\n",
    "Implement the `update` function to adjust the value estimates based on received rewards. For the learning rate $\\alpha_t$, use the formula $\\alpha_t = \\frac{1}{\\text{count}(A_t)}$.\n",
    "\n",
    "_Pro Tips:_ If you hold down <kbd>cmd</kbd> + click the `update` in VSCode, it will bring you to the `bandits.py - update`. In general, if you want to go to definition of any methods, this will do the trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Action Selection\n",
    "\n",
    "Create a function to select the action with the highest estimated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Epsilon-Greedy Action Selection\n",
    "\n",
    "Modify the existing greedy function to include an exploration factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import egreedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating Bandit Strategies\n",
    "\n",
    "### Simulation Setup\n",
    "\n",
    "In this task, we will simulate the maximum value from bandits over multiple trials. The bandits represent the different slot machines in the multi-armed bandit problem. Each bandit has a reward distribution that is specific to that machine but unknown to the agent. The goal is to determine the bandit with the highest payout rate through trial and experience.\n",
    "\n",
    "To simulate the bandit strategies, we will first generate a set of bandits with rewards drawn from a normal distribution, and find the maximum value from the bandits over multiple trials. The reward for each arm is drawn from a normal distribution with a mean of 0 and a standard deviation of 1, using the `np.random.normal` function.\n",
    "\n",
    "For each trial, we can find the maximum value from the bandits by selecting the arm with the highest estimated value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxv = np.empty(10000)\n",
    "for idx in range(10000):\n",
    "    bandits = np.random.normal(size=n_arms)\n",
    "    maxv[idx] = np.max(bandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Maximum Values\n",
    "The cell below visualizes the distribution of maximum $Q$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(maxv)\n",
    "mave = np.mean(maxv)\n",
    "plt.title(f\"Max Q of 10 armed bandit over 10k trials: mean={mave:.3f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Experiment and Analysis\n",
    "\n",
    "### Running the Experiments\n",
    "\n",
    "Now, we will run experiments using different strategies to solve the multi-armed bandit problem. We will compare the performance of the greedy and epsilon-greedy algorithms by plotting the average rewards over time. \n",
    "\n",
    "Similar to the previous task, the reward for each arm is drawn from a normal distribution with a mean of 0 and a standard deviation of 1, via the `np.random.normal` function.\n",
    "\n",
    "Your task is to complete the code below by calling the `egreedy` function that you defined earlier, and perform experiments with **three** different epsilon values for the epsilon-greedy strategy. Your list of epsilon values should include 0 (greedy strategy) and two other values of your choice that are in the range $\\epsilon \\in (0, 0.3]$.\n",
    "\n",
    "You can expect this cell to take at least 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bandits import empirical_egreedy\n",
    "\n",
    "data = []\n",
    "epsilon_values = [0, 0.01, 0.1]\n",
    "for epsilon in epsilon_values:\n",
    "    rewards = ... # YOUR CODE HERE\n",
    "    df = pd.DataFrame(rewards).reset_index().melt(id_vars=[\"index\"])\n",
    "    df.rename(columns={\"index\": \"trials\", \"variable\": \"plays\", \"value\": \"reward\"}, inplace=True)\n",
    "    if epsilon == 0:\n",
    "        df[\"epsilon\"] = \"greedy\"\n",
    "    else:\n",
    "        df[\"epsilon\"] = f\"$\\\\epsilon={epsilon}$\"\n",
    "    data.append(df)\n",
    "alldata = pd.concat(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Strategy Performance\n",
    "\n",
    "We can visualize and compare the performance of different strategies by running the cell below. This might take a little more than 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.lineplot(data=alldata, x=\"plays\", y=\"reward\", hue=\"epsilon\", palette=\"Set2\")\n",
    "plt.axhline(mave, label=\"Mean max value\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Bandit Strategies Over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Performance Analysis\n",
    "\n",
    "We can analyze the early game performance using line plots for smaller number of plays. Plot the initial 100 plays of each trails for each epsilon values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "plt.title(\"Early Game Analysis of Bandit Strategies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualizing Bandit Strategies for a Slot Machine Simulator\n",
    "\n",
    "The next part of this assignment involves using `slot_machine.py`. You will work with a simple slot machine simulator that models a multi-armed bandit problem using the $\\epsilon$-greedy algorithm. The goal is to maximize your rewards by selecting slot machines based on past rewards and an exploration strategy that balances the trade-off between exploiting the best-known machine and exploring other machines.\n",
    "\n",
    "**Note**: for this part, you will need to run the code in a local environment with a graphical interface (e.g., your local machine), similar to how you did L2 and L3. The code will not run in the notebook environment.\n",
    "\n",
    "### Problem Statement\n",
    "You are faced with `n` slot machines, each with a different probability of paying out a reward. These probabilities are unknown to you at the start. You can play one machine per round, and each play has a cost. Your objective is to maximize your net rewards over a series of plays.\n",
    "\n",
    "### Environment Setup\n",
    "The simulator is set up using Pygame, a popular library for writing video games in Python. The environment consists of:\n",
    "\n",
    "- **n_machines**: Number of slot machines.\n",
    "- **true_rewards**: Array storing the true probability of each machine giving a reward.\n",
    "- **estimated_rewards**: Array to maintain our estimates of each machine's reward probabilities.\n",
    "- **play_counts**: Array tracking how many times each machine has been played.\n",
    "- **cost_per_play**: Cost incurred each time a machine is played.\n",
    "- **total_reward**: Net reward accumulated over all plays.\n",
    "- **epsilon**: Probability of choosing to explore (i.e., selecting a machine at random).\n",
    "\n",
    "### Code Structure\n",
    "The code is organized as follows:\n",
    "\n",
    "- **Initialization**: Set up the Pygame environment, define screen dimensions, colors, fonts, and initialize game settings.\n",
    "- **Game Loop**:\n",
    "  - **Event Handling**: Process mouse clicks for selecting machines in manual mode.\n",
    "  - **AI Mode Logic**: Implement the epsilon-greedy algorithm to select which machine to play based on the estimated rewards and exploration factor.\n",
    "  - **Reward Calculation**: Update the total rewards based on the machine's payout and the cost per play.\n",
    "  - **Display Updates**: Render the slot machines, number of plays, and total rewards on the screen.\n",
    "\n",
    "## Tasks to Complete\n",
    "To complete this assignment, you need to fill in the missing parts of the code that are marked with `# TODO`. The main tasks are as follows:\n",
    "\n",
    "1. **Update Estimated Rewards**: After playing a machine, update your estimate of its reward probability based on the outcome.\n",
    "2. **Implement Epsilon-Greedy Selection**:Randomly decide (based on epsilon) whether to explore (choose a machine randomly) or exploit (choose the best-known machine).\n",
    "3. **Simulate Reward Collection:** Simulate obtaining a reward from the chosen machine based on its true reward probability.\n",
    "4. **Update Play Counts and Total Reward:** Update the count of plays for the selected machine and adjust the total reward based on whether the machine paid out.\n",
    "5. **Incremental Update Formula:** Use the incremental update formula to update the estimated reward probability after each play.\n",
    "\n",
    "Before you begin the assignment, you can get a feel of how the game works by running the code in \"manual mode\" and playing a few rounds, as follows:\n",
    "\n",
    "```bash\n",
    "python slot_machine.py --mode manual\n",
    "```\n",
    "\n",
    "This will open a window with the slot machines, and you can click on the machines to begin accumulating reward. The game will display the number of plays on each machine and the total reward. \n",
    "\n",
    "After you implement the TODOs in the code, you can then run the game in \"AI mode\" to see how the $\\epsilon$-greedy algorithm performs:\n",
    "\n",
    "```bash\n",
    "python slot_machine.py --mode AI --epsilon 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how the algorithm balances exploration and exploitation as you observe the game in action. You can also experiment with different values of $\\epsilon$ to see how it affects the agent's behavior. Does the AI mode perform better than the manual mode? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit `bandits.py` and `slot_machine.py` to gradescope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
