{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Monte Carlo Methods & DP (2 points)\n",
    "\n",
    "**Deadline**: Tuesday, May 21, 2024 at 11:59pm on Gradescope\n",
    "\n",
    "## Installations\n",
    "\n",
    "Ensure you have the following modules installed on your system:\n",
    "\n",
    "* Numpy\n",
    "* Matplotlib\n",
    "\n",
    "You can install these modules using pip:\n",
    "\n",
    "```bash\n",
    "pip install numpy matplotlib\n",
    "```\n",
    "\n",
    "## Part 1: Estimating $\\pi$ using Simple Monte Carlo Methods [1 point]\n",
    "\n",
    "This part involves using the Monte Carlo method to estimate the value of $\\pi$. You will simulate random point placements within a square to determine how many points fall inside a unit circle inscribed in the square. This method leverages probability and the geometric properties of the circle and the square to approximate the value of π.\n",
    "\n",
    "Note that the area of a unit circle is $\\pi \\times 1^2 = \\pi$. This unit circle is inscribed in a square with side length 2, so the area of the square is $2^2 = 4$. The ratio of the area of the circle to the area of the square is $\\frac{\\pi}{4}$. Therefore, the probability of a point falling inside the circle is $\\frac{\\pi}{4}$.\n",
    "\n",
    "You are provided with a function template called `estimate_pi(num_samples, step)`. This function should simulate the placement of `num_samples` random points within a unit square and determine how many fall inside the inscribed unit circle.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "* Implement the Monte Carlo logic inside the `estimate_pi` function to estimate $\\pi$.\n",
    "* Collect data periodically (as specified by the step parameter) for visualization purposes. This data should include:\n",
    "  * Coordinates of points inside the circle.\n",
    "  * Coordinates of points outside the circle.\n",
    "  * The current estimate of $\\pi$.\n",
    "\n",
    "After implementing the functions, test your simulation with different numbers of samples and steps. Observe the accuracy and variability of your $\\pi$ estimates over time.\n",
    "\n",
    "The resulting visualization should be saved as a GIF file named `monte_carlo_pi.gif`. The GIF should show the progression of the Monte Carlo simulation over time with the estimated value of $\\pi$ displayed on each frame. If you implemented it correctly, the estimated value of $\\pi$ should converge to the actual value of $\\pi = 3.14159...$ as the number of samples increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimate_pi import estimate_pi\n",
    "\n",
    "data = estimate_pi(100, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimate_pi import create_animation_pi\n",
    "from IPython.display import Image\n",
    "\n",
    "# TODO: implement create_animation_pi, and run it with 1e5 samples\n",
    "create_animation_pi\n",
    "Image(\"monte_carlo_pi.gif\")\n",
    "\n",
    "# submit the animation to gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Dynamic Programming (DP) to solve Black Jack [1 point]\n",
    "\n",
    "Blackjack is a popular card game played in casinos where the goal is to beat the dealer by having a hand value closer to 21 than the dealer’s hand without exceeding 21.\n",
    "\n",
    "## Game Rule\n",
    "\n",
    "### Card Values\n",
    "\n",
    "- Number cards (2-10): Worth their face value.\n",
    "- Face cards (J, Q, K): Worth 10 points each.\n",
    "- Aces (A): Worth either 1 or 11 points, whichever benefits the player without exceeding 21.\n",
    "\n",
    "### Game Setup\n",
    "\n",
    "1. Each player and the dealer are dealt two cards.\n",
    "    - Players’ cards are both face-up.\n",
    "    - The dealer has one card face-up (the “upcard”) and one face-down (the “hole card”).\n",
    "2. Players take turns deciding their actions before the dealer plays.\n",
    "\n",
    "### Player Actions\n",
    "\n",
    "After receiving the initial two cards, the player can choose one of the following actions:\n",
    "1.\tHit → Take another card to increase the hand total.\n",
    "2.\tStand (Stick) → Stop taking cards and keep the current hand total.\n",
    "3.\tDouble Down → Double the bet and take exactly one more card.\n",
    "4.\tSplit → If both initial cards are the same rank, split them into two separate hands, each with an additional bet.\n",
    "5.\tSurrender (if allowed) → Forfeit the round and get half of the bet back.\n",
    "6.\tInsurance (if dealer shows Ace) → A side bet that the dealer has Blackjack (pays 2:1).\n",
    "\n",
    "### Dealer’s Turn\n",
    "\n",
    "Once all players have completed their actions:\n",
    "- The dealer reveals their hidden card.\n",
    "- The dealer must draw until reaching at least 17:\n",
    "- If the total is 16 or less, the dealer must draw another card.\n",
    "- If the total is 17 or more, the dealer must stand.\n",
    "- Some casinos require the dealer to hit on a “soft 17” (Ace + 6).\n",
    "\n",
    "### Winning and Losing\n",
    "\n",
    "Once the dealer finishes playing, hands are compared:\n",
    "- Blackjack (Natural 21)\n",
    "- A player with an Ace + 10-point card on the first two cards wins automatically unless the dealer also has a Blackjack (resulting in a tie).\n",
    "- A Blackjack usually pays 3:2 (e.g., a $10 bet wins $15).\n",
    "- Player Wins\n",
    "- The player has a higher total than the dealer (without exceeding 21).\n",
    "- The dealer busts (exceeds 21).\n",
    "- Standard win pays 1:1.\n",
    "- Player Loses\n",
    "- The player busts (exceeds 21).\n",
    "- The dealer’s total is higher than the player’s.\n",
    "- Push (Tie)\n",
    "- The player and dealer have the same total.\n",
    "- The bet is returned to the player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.blackjack import Card, Hand, BlackjackEnv\n",
    "\n",
    "# create a cards list with 10 cards, from 1 to 10\n",
    "cards = [Card(i + 1) for i in range(10)]\n",
    "\n",
    "hand = Hand()\n",
    "hand.add_card(cards[2])\n",
    "hand.add_card(cards[7])\n",
    "# now the hand is [3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the hands and hand value\n",
    "hand, hand.get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new environment\n",
    "env = BlackjackEnv()\n",
    "\n",
    "# take a look at the initial state\n",
    "next_state, reward, done = env.reset()  # (next_state, reward, done)\n",
    "\n",
    "print(\"Initial State:\", next_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Formulations\n",
    "\n",
    "For policy evaluation or value iteration, we typically define a value function:\n",
    "\n",
    "$$\n",
    "V(s) = \\mathbb{E}[ \\text{return} \\mid s ]\n",
    "$$\n",
    "\n",
    "State space in Blackjack can be enumerated by:\n",
    "- player_sum in $[4..21]$ (although typically we are more interested in $[12..21]$ because if your sum is < 12, you almost always want to hit).\n",
    "- dealer_upcard in $[1..10]$.\n",
    "- usable_ace in $\\{True, False\\}$.\n",
    "\n",
    "Transitions:\n",
    "- From state s, if the action is `HIT`:\n",
    "    - We draw a new card. That leads us to a new state s' or we bust.\n",
    "    - If we bust, we get a reward of -1 and transition to a terminal state.\n",
    "- From state s, if the action is `STICK`:\n",
    "    - The dealer draws. The outcome can be dealer bust (player wins, reward = +1) or final comparison (reward = +1, 0, or -1).\n",
    "\n",
    "Expected Returns: we can compute expected returns by averaging over possible next cards (which appear with some probability).\n",
    "\n",
    "## Policy Evaluation (Iterative)\n",
    "\n",
    "3. Policy Evaluation (Iterative)\n",
    "\n",
    "Policy Evaluation (a.k.a. Iterative Policy Evaluation) aims to solve:\n",
    "\n",
    "\n",
    "$$\n",
    "V_{k+1}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s{\\prime}, r} p(s{\\prime}, r \\mid s, a) \\big[ r + \\gamma V_k(s{\\prime}) \\big]\n",
    "$$\n",
    "\n",
    "In a standard Blackjack scenario, we typically take $\\gamma = 1$ because it’s a finite episode with no ongoing discount, but you can also use $\\gamma < 1$ if you want.\n",
    "\n",
    "Algorithmically:\n",
    "1. Initialize $V(s)$ to 0 (or random values) for all states.\n",
    "2. Loop:\n",
    "    - For each state $s$:\n",
    "    - Calculate the new value:\n",
    "\n",
    "\n",
    "$$\n",
    "V_{new}(s) = \\mathbb{E}_{a \\sim \\pi(\\cdot|s), s{\\prime}, r \\sim p(\\cdot|s, a)}\\left[ r + \\gamma V(s{\\prime}) \\right]\n",
    "$$\n",
    "\n",
    "- Update $V(s) ← V_{new}(s)$.\n",
    "- Stop when changes are below some threshold or after a fixed number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DP table\n",
    "import numpy as np\n",
    "\n",
    "# Constants / ranges\n",
    "PLAYER_SUM_RANGE = range(4, 22)  # Though typically 12..21 is the main range\n",
    "DEALER_UPCARD_RANGE = range(1, 11)\n",
    "USABLE_ACE_RANGE = [False, True]\n",
    "ACTIONS = [0, 1]  # 0->HIT, 1->STICK\n",
    "\n",
    "from src.policy import initialize_value\n",
    "\n",
    "V = initialize_value()\n",
    "total_states = len(V)  # total number of states\n",
    "print(f\"Total number of states: {total_states}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a simple policy to start with. For example, we can always stick if we have a sum more than 18, and we can always hit if we have a sum of 18 or less. This is a deterministic policy. Implement that policy in the `simple_policy` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.policy import simple_policy\n",
    "\n",
    "policy_table = simple_policy()\n",
    "\n",
    "\n",
    "# this means that at state 4, 1, 0, the policy is to HIT\n",
    "state = (4, 1, 0)\n",
    "action = policy_table[state]\n",
    "print(f\"The action for state {state} is: {'HIT' if action == 0 else 'STICK'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Iterative)\n",
    "\n",
    "We’ll do a simplified approach to policy evaluation. We can sample transitions or do a full expected update. Here, we show a Monte Carlo style sampling approach for policy evaluation (which is conceptually simpler, though not purely DP). For a purely dynamic-programming approach, you would compute the exact expectation by enumerating next card probabilities. In practice for Blackjack, this is typically feasible because there are only 13 card ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dp import policy_evaluation\n",
    "\n",
    "env = BlackjackEnv()\n",
    "V = initialize_value()\n",
    "policy = simple_policy()\n",
    "\n",
    "V = policy_evaluation(env, V, policy, episodes=500_000, gamma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the value generated by the policy evaluation, we can visualize the value function as a line graph based on the player sum. The x-axis should represent the player sum, while the y-axis should represent the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Create a list of all possible player sums from the defined PLAYER_SUM_RANGE\n",
    "player_sums = list(PLAYER_SUM_RANGE)\n",
    "\n",
    "# Initialize an empty list to store the average value for each player sum\n",
    "avg_values = []\n",
    "\n",
    "# Iterate over every player sum in the range\n",
    "for psum in player_sums:\n",
    "    vals = []  # This list will collect the value function results for a fixed player sum over all dealer upcards and usable ace states\n",
    "    \n",
    "    # Loop through each possible dealer upcard value\n",
    "    for dealer in DEALER_UPCARD_RANGE:\n",
    "        # Loop through each usable ace state (True or False)\n",
    "        for ace in USABLE_ACE_RANGE:\n",
    "            state = (psum, dealer, ace)  # Define the current state as a tuple\n",
    "            if state in V:\n",
    "                # If the state exists in the value function dictionary V, append its value\n",
    "                vals.append(V[state])\n",
    "    \n",
    "    # Calculate the average value for the current player sum\n",
    "    # If there are values available, compute the average; otherwise, default to 0\n",
    "    avg = sum(vals) / len(vals) if vals else 0\n",
    "    # Append the computed average to the avg_values list\n",
    "    avg_values.append(avg)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(player_sums, avg_values, marker=\"o\")\n",
    "plt.axvline(x=17.5, color=\"r\", linestyle=\"--\", label=\"Policy Change (Hit to Stand)\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Player Sum\")\n",
    "plt.ylabel(\"Average Value\")\n",
    "plt.title(\"Average Value for Player Hands\")\n",
    "# submit this file to gradescope\n",
    "plt.savefig(\"avg_values.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ungraded:__ Interpret the value graph in above in relationship between the player sum and the policy we created.\n",
    "\n",
    "_Response:_ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing:\n",
    "\n",
    "You can test your implementation by running the following command in your directory that contains the `L6/tests`:\n",
    "\n",
    "```bash\n",
    "python -m unittest discover -s tests\n",
    "```\n",
    "\n",
    "# Submission\n",
    "\n",
    "You need to submit the following files to Gradescope:\n",
    "\n",
    "1. `monte_carlo_pi.gif`\n",
    "2. `avg_value.png`\n",
    "3. `blackjack.py`\n",
    "4. `dp.py`\n",
    "5. `policy.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
